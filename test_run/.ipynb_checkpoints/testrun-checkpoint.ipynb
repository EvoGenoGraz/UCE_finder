{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First reformat Mummer output to a human readable txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!../reformating_mummer_results/ReformatMUMMERoutput.sh ../raw_data/LvSf80.out LvSf80.out.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reformat_mummer(mummer_out_file):\n",
    "    \n",
    "    outlist = []\n",
    "    temp_list = []\n",
    "    \n",
    "    for line in open(mummer_out_file, 'r'):\n",
    "        line=line.strip()\n",
    "        if line.startswith(\">\"):\n",
    "            temp_list = [line.replace(\"> \",\"\")]\n",
    "        else:\n",
    "            temp_list.append('\\t'.join(line.split()))\n",
    "            if len(temp_list) == 3:\n",
    "                outlist.append(\"\\t\".join(temp_list))\n",
    "                temp_list = [temp_list[0]]\n",
    "            \n",
    "    return outlist\n",
    "            \n",
    "            \n",
    "def output_mummer_tabular(outfile, result_list):\n",
    "    \n",
    "    fh = open(outfile,'w')\n",
    "    for res in result_list:\n",
    "        fh.write(res+'\\n')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ctg_lengths(fasta):\n",
    "    \n",
    "    from Bio import SeqIO\n",
    "    lengths = {}\n",
    "\n",
    "    for seq in SeqIO.parse(open(fasta,'r'),'fasta'):\n",
    "        lengths[seq.id]=len(seq.seq)\n",
    "        \n",
    "    return lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_dist_from_end(dist_from_edge, pos, mode, total_length=0):\n",
    "    \n",
    "    import sys\n",
    "    \n",
    "    ok = True\n",
    "    \n",
    "#    dist_from_edge = int(dist_from_edge)\n",
    "#    pos=int(p)\n",
    "    if mode == 'start':\n",
    "        if not pos >= (dist_from_edge): #if match is not far enough from the start of the sequence\n",
    "            ok = False\n",
    "        \n",
    "    elif mode == 'end':\n",
    "     \n",
    "        if not total_length > 0:\n",
    "            sys.exit('need to know the length of the contig')\n",
    "            \n",
    "        if not (total_length-pos) >= dist_from_edge:\n",
    "            ok = False\n",
    "        \n",
    "    if ok:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lis = get_ctg_lengths('../raw_data/Lvar_final.contigs.fa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist=10\n",
    "p=3154\n",
    "ctg_id='k101_421826'\n",
    "print lis[ctg_id]\n",
    "evaluate_dist_from_end(dist_from_edge=dist, pos=p, mode='end', total_length=lis[ctg_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reformat_match(ref_id, ref_start_pos, baselist):\n",
    "    \n",
    "    #ref_id is the unique id of the reference contig\n",
    "    #ref_start_pos is the coordinate of the start position on the reference\n",
    "    #baselist is a list containing [query_id, query_start_pos, match_length, sequence]\n",
    "    #returns a dictionary of format:\n",
    "    #{'ref':{'id':'xyz', 'start': '123', 'end': '234', 'or':'+', 'seq':'AGCT'}, \n",
    "    #'que':{'id':'xyz', 'start': '123', 'end': '234', 'or':'-', 'seq':'AGCT'}}\n",
    "    \n",
    "    temp_dict = {'ref':{}, 'que':{}}\n",
    "    \n",
    "    temp_dict['ref']['id'] = ref_id\n",
    "    temp_dict['ref']['start'] = ref_start_pos\n",
    "    temp_dict['ref']['end'] = ref_start_pos + int(baselist[2])\n",
    "    temp_dict['ref']['seq'] = baselist[3].upper()\n",
    "    temp_dict['ref']['or'] = '+'\n",
    "    temp_dict['que']['id'] = baselist[0]\n",
    "    temp_dict['que']['start'] = int(baselist[1])\n",
    "    temp_dict['que']['end'] = int(baselist[1]) + int(baselist[2])\n",
    "    temp_dict['que']['seq'] = baselist[3].upper()\n",
    "    temp_dict['que']['or'] = '+'\n",
    "                \n",
    "    if baselist[0].endswith('Reverse'):\n",
    "        temp_dict['que']['id'] = temp_dict['que']['id'].replace('_Reverse','')\n",
    "        temp_dict['que']['or'] = '-'\n",
    "\n",
    "    return temp_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write report for the binned matches to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_binned_match_details(outfile, master_dict):\n",
    "    \n",
    "    order = ['id','start','end','or','seq']\n",
    "        \n",
    "    ## Write out coordinates of candidates\n",
    "    outfh = open(outfile,'w')\n",
    "    for group in master_dict.keys():\n",
    "        for uce in master_dict[group]:\n",
    "            linestart = \"%s\\t%s\\t\" %(uce,group)\n",
    "            if isinstance(master_dict[group][uce], list):\n",
    "#                print uce,len(binned[group][uce])\n",
    "                for rep in master_dict[group][uce]:\n",
    "                    temp = []\n",
    "                    for target in ['ref','que']:\n",
    "                        for key in order:\n",
    "                            temp.append(str(rep[target][key]))\n",
    "                    line=linestart+\"\\t\".join(temp)\n",
    "                    outfh.write(line+'\\n')\n",
    "            else:\n",
    "                temp = []\n",
    "                for target in ['ref','que']:\n",
    "                    for key in order:\n",
    "                        temp.append(str(master_dict[group][uce][target][key]))\n",
    "                line=linestart+\"\\t\".join(temp)\n",
    "                outfh.write(line+'\\n')\n",
    "    \n",
    "    outfh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write valid UCE coordinates to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_UCE_coordinates(outfile, master_dict):\n",
    "    \n",
    "    order = ['id','start','end','or','seq']\n",
    "        \n",
    "    ## Write out coordinates of candidates\n",
    "    outfh = open(outfile,'w')\n",
    "    for group in master_dict.keys():\n",
    "        for uce in master_dict[group]:\n",
    "            line = \"%s\\t%s\\t\" %(uce,group)\n",
    "            temp = []\n",
    "            for target in ['ref','que']:\n",
    "                for key in order:\n",
    "                    temp.append(str(master_dict[group][uce][target][key]))\n",
    "            line+=\"\\t\".join(temp)\n",
    "            outfh.write(line+'\\n')\n",
    "    \n",
    "    outfh.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract UCE sequences to file with or without extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_UCEs(prefix, candidates, target='ref', extension=0, fasta='', minlength=0):\n",
    "    \"\"\"\n",
    "    Write UCE's to fasta file optionally including and extension of x bp up and downstream of the UCE\n",
    "    \"\"\"\n",
    "\n",
    "    import sys\n",
    "    from Bio import SeqIO\n",
    "\n",
    "    if extension and not fasta:\n",
    "        sys.exit(\"\\nYou want to extract UCE's with and extension - need a fasta file for that\\n\")\n",
    "    \n",
    "    if not extension:\n",
    "        outfh = open(prefix+'.fasta','w')\n",
    "        for group in candidates.keys():\n",
    "            for uce in candidates[group]:\n",
    "                if not len(candidates[group][uce]['ref']['seq']) >= minlength:\n",
    "                        continue\n",
    "                header_suffix = '%s|%s|%s' %(uce, candidates[group][uce][target]['id'], candidates[group][uce][target]['start'])\n",
    "                outfh.write(\">%s|%s\\n%s\\n\" %(group, header_suffix, candidates[group][uce][target]['seq']))\n",
    "    \n",
    "        outfh.close()\n",
    "    \n",
    "    else:\n",
    "        #identify ref contigs for which I need to extract sequences\n",
    "        ctgs = {}\n",
    "\n",
    "        for group in candidates.keys():\n",
    "            for uce in candidates[group]:\n",
    "                \n",
    "                ctg_id = candidates[group][uce][target]['id']\n",
    "                if not ctg_id in ctgs:\n",
    "                    ctgs[ctg_id] = []\n",
    "                ctgs[ctg_id].append(uce+' '+group)\n",
    "        \n",
    "        \n",
    "        outfh = open(prefix+'.fasta','w')\n",
    "        for seq in SeqIO.parse(open(fasta,'r'),'fasta'):\n",
    "            if seq.id in ctgs:\n",
    "        #        print seq.id\n",
    "                for uce_with_group in ctgs[seq.id]:\n",
    "                    (uce,group) = uce_with_group.split(\" \")\n",
    "                    if not len(candidates[group][uce]['ref']['seq']) >= minlength:\n",
    "                        continue\n",
    "                        \n",
    "                    startadd = ''\n",
    "                    endadd = ''\n",
    "                    \n",
    "        #            print uce,group\n",
    "                    start = candidates[group][uce][target]['start']-extension-1\n",
    "                    end = candidates[group][uce][target]['end']+extension-1\n",
    "                    if start < 0:\n",
    "#                        print group,uce,candidates[group][uce][target]\n",
    "                        startadd = 'N' * (start*-1)\n",
    "                        start = 0\n",
    "                \n",
    "                    if end > len(seq.seq):\n",
    "    #                    print uce,candidates[group][uce][target]\n",
    "    #                    print end,len(seq.seq)\n",
    "                        endadd = 'N' * (end-len(seq.seq))\n",
    "                    \n",
    "                    if candidates[group][uce][target]['or'] == '+':\n",
    "                        uc_seq = str(seq.seq[start:end])\n",
    "                    else:\n",
    "                        uc_seq = str(seq.seq.reverse_complement()[start:end])\n",
    "        #            print \"\\t%s,%s,%s\" %(uce, candidates['merged'][uce]['que']['start'], candidates['merged'][uce]['que']['end'])\n",
    "                    \n",
    "                    header_suffix = '%s|%s|%s' %(uce, candidates[group][uce][target]['id'], candidates[group][uce][target]['start'])\n",
    "                    outfh.write(\">%s|%s\\n%s\\n\" %(group, header_suffix, candidates[group][uce][target]['seq']))\n",
    "                    outfh.write(\">%s|%s\\n%s%s%s\\n\" %(group, header_suffix, startadd, uc_seq, endadd))\n",
    "    \n",
    "        outfh.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then process the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LvSf80.out.tsv\n",
      "Total number of MUMMER matches: 1482\n",
      "Filter 'multiple query hits with identical start position on reference' removed 26 Mummer hits\n",
      "Leaves us with 1456 hits to process\n",
      "\n",
      "MERGING\n",
      "\n",
      "\n",
      "too-close\t7\n",
      "skip\t180\n",
      "overlap-filter\t6\n",
      "merge-dist-diff\t1\n",
      "position\t11\n",
      "irregular-merge\t0\n",
      "\n",
      "unique\t153\n",
      "singlematch\t1032\n",
      "merged\t30\n",
      "\n",
      "unique\t153\n",
      "80 5\n",
      "81 6\n",
      "82 5\n",
      "83 4\n",
      "84 6\n",
      "85 5\n",
      "86 2\n",
      "87 3\n",
      "88 2\n",
      "89 5\n",
      "90 4\n",
      "91 2\n",
      "92 2\n",
      "93 5\n",
      "94 4\n",
      "95 2\n",
      "97 4\n",
      "98 4\n",
      "99 2\n",
      "100 4\n",
      "101 1\n",
      "102 4\n",
      "103 2\n",
      "104 2\n",
      "105 2\n",
      "106 3\n",
      "107 1\n",
      "108 3\n",
      "109 2\n",
      "111 1\n",
      "112 4\n",
      "113 2\n",
      "114 3\n",
      "115 3\n",
      "116 1\n",
      "120 2\n",
      "121 1\n",
      "124 1\n",
      "125 1\n",
      "127 3\n",
      "128 2\n",
      "129 1\n",
      "131 2\n",
      "132 6\n",
      "133 2\n",
      "134 1\n",
      "136 1\n",
      "137 2\n",
      "140 1\n",
      "141 1\n",
      "152 1\n",
      "154 1\n",
      "157 1\n",
      "166 1\n",
      "170 1\n",
      "175 1\n",
      "176 1\n",
      "180 1\n",
      "195 1\n",
      "197 1\n",
      "209 1\n",
      "221 1\n",
      "247 2\n",
      "309 1\n",
      "392 1\n",
      "\n",
      "singlematch\t1032\n",
      "80 73\n",
      "81 54\n",
      "82 44\n",
      "83 38\n",
      "84 32\n",
      "85 46\n",
      "86 43\n",
      "87 37\n",
      "88 31\n",
      "89 35\n",
      "90 24\n",
      "91 21\n",
      "92 25\n",
      "93 21\n",
      "94 18\n",
      "95 37\n",
      "96 25\n",
      "97 19\n",
      "98 20\n",
      "99 12\n",
      "100 17\n",
      "101 24\n",
      "102 15\n",
      "103 16\n",
      "104 12\n",
      "105 9\n",
      "106 14\n",
      "107 16\n",
      "108 13\n",
      "109 13\n",
      "110 9\n",
      "111 8\n",
      "112 9\n",
      "113 12\n",
      "114 6\n",
      "115 13\n",
      "116 7\n",
      "117 7\n",
      "118 7\n",
      "119 10\n",
      "120 4\n",
      "121 7\n",
      "122 5\n",
      "123 9\n",
      "124 5\n",
      "125 8\n",
      "126 5\n",
      "127 3\n",
      "128 8\n",
      "129 6\n",
      "130 3\n",
      "131 1\n",
      "134 4\n",
      "135 1\n",
      "136 3\n",
      "138 4\n",
      "139 2\n",
      "140 1\n",
      "141 2\n",
      "142 1\n",
      "143 1\n",
      "144 3\n",
      "145 1\n",
      "146 6\n",
      "147 3\n",
      "148 3\n",
      "149 2\n",
      "150 4\n",
      "151 3\n",
      "153 2\n",
      "154 2\n",
      "155 1\n",
      "157 1\n",
      "160 1\n",
      "161 2\n",
      "162 3\n",
      "163 1\n",
      "164 1\n",
      "165 1\n",
      "168 1\n",
      "169 1\n",
      "173 1\n",
      "176 2\n",
      "177 1\n",
      "179 1\n",
      "185 2\n",
      "190 1\n",
      "191 1\n",
      "194 1\n",
      "195 1\n",
      "201 1\n",
      "243 1\n",
      "274 1\n",
      "438 1\n",
      "\n",
      "merged\t30\n",
      "171 1\n",
      "172 1\n",
      "179 1\n",
      "180 1\n",
      "189 1\n",
      "190 1\n",
      "191 1\n",
      "192 1\n",
      "194 1\n",
      "196 2\n",
      "197 1\n",
      "202 1\n",
      "207 1\n",
      "210 1\n",
      "211 2\n",
      "212 1\n",
      "216 1\n",
      "219 1\n",
      "220 1\n",
      "223 1\n",
      "232 1\n",
      "233 1\n",
      "252 1\n",
      "282 1\n",
      "326 1\n",
      "328 1\n",
      "339 1\n",
      "535 1\n",
      "\n",
      "WRITING GLOBAL OUTPUTS with prefix: 'LvSf80.out'\n",
      "\n",
      "\n",
      "WRITING: Lvar_final.contigs.candidate_UCEs-minlength100.fasta\n",
      "\n",
      "WRITING: Sfran_final.contigs.candidate_UCEs-minlength100.fasta\n",
      "\n",
      "WRITING: Lvar_final.contigs.candidate_UCEs-minlength100-extended500.fasta\n",
      "\n",
      "WRITING: Sfran_final.contigs.candidate_UCEs-minlength100-extended500.fasta\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mummer_result = \"../raw_data/LvSf80.out\" #format of the input here was defined by us (see separate workflow)\n",
    "#Tab delimited file\n",
    "#6 columns\n",
    "#col 1: query contig id\n",
    "#col 2: REf contig id\n",
    "#col 3: ref start pos\n",
    "#col 4: query start pos\n",
    "#col 5: length of match\n",
    "#col 6: nucleotide sequence\n",
    "#ref = subject in mummer, i.e. the fasta that was specified first in the mummer command\n",
    "#query = query in mummer, i.e. the fasta that was specified second in the mummer command\n",
    "#example mummer command: mummer -maxmatch -l 120 -F ref.fasta query.fas\n",
    "#The input file was created from mummer output with a custom script\n",
    "\n",
    "#Input files - reference and query genome assembly\n",
    "quefasta = '../raw_data/Sfran_final.contigs.fa'\n",
    "reffasta = '../raw_data/Lvar_final.contigs.fa'\n",
    "\n",
    "max_merge_distance = 40 #maximum distance between 2 unique perfect matches from mummer to be merged to a single bait\n",
    "unique_distance = 1000 #minimum distance between 2 candidates (if distance is shorter only consider the longer perfect match)\n",
    "dist_from_edge = 10\n",
    "min_diff = 0 #negative value means that the gap on the query is smaller than on the reference\n",
    "max_diff = 10 #maximum gap size difference between ref and query in a case of adjacent candidates \n",
    "\n",
    "extension=500\n",
    "minlength=100\n",
    "\n",
    "###########################################\n",
    "\n",
    "mummerlist = reformat_mummer(mummer_result)\n",
    "\n",
    "mummer_result_prefix = mummer_result.split(\"/\")[-1]\n",
    "print mummer_result_prefix+'.tsv'\n",
    "output_mummer_tabular(mummer_result_prefix+'.tsv', mummerlist)\n",
    "\n",
    "\n",
    "mummer_fh = open(mummer_result_prefix+'.tsv','r')\n",
    "#mummer_fh = open('../reformating_mummer_results/test.out.tsv','r')\n",
    "\n",
    "per_ref_ctg = {}\n",
    "total = 0\n",
    "\n",
    "candidates = {'singlematch':{}, 'unique':{}, 'merged':{}}\n",
    "binned = {'position':{}, 'too-close':{}, 'overlap-filter':{}, 'skip': {}, 'irregular-merge':{}, 'merge-dist-diff':{}}\n",
    "\n",
    "\n",
    "que_prefix = \".\".join(quefasta.split(\"/\")[-1].split(\".\")[:-1])\n",
    "ref_prefix = \".\".join(reffasta.split(\"/\")[-1].split(\".\")[:-1])\n",
    "global_prefix = mummer_result_prefix\n",
    "\n",
    "lengths = get_ctg_lengths(reffasta)\n",
    "\n",
    "#read in reformatted mummer output\n",
    "#The loop produces a dictionary with the following structure:\n",
    "#key = reference contig id\n",
    "#value = dictionary with key = start position on reference and value is list with [queryid, querystart, matchlength, sequence]\n",
    "\n",
    "for line in mummer_fh:\n",
    "    llist = line.strip().split(\"\\t\")\n",
    "    total+=1\n",
    "#    print llist\n",
    "    if not llist[1] in per_ref_ctg:\n",
    "        per_ref_ctg[llist[1]] = {}\n",
    "    if not int(llist[2]) in per_ref_ctg[llist[1]]:\n",
    "        per_ref_ctg[llist[1]][int(llist[2])] = []\n",
    "    per_ref_ctg[llist[1]][int(llist[2])].append([llist[0],llist[3],llist[4],llist[5]])\n",
    "\n",
    "print \"Total number of MUMMER matches: %i\" %total\n",
    "#print per_ref_ctg    \n",
    "    \n",
    "#Filter 'multiple query hits with identical start position on reference'\n",
    "#searches for multiple matches with the same startposition on the same reference contig, i.e. more than one matches with the query start at the \n",
    "#same position in the reference\n",
    "\n",
    "filtercount=0\n",
    "for ref_ctg_id in sorted(per_ref_ctg):\n",
    "#    print ref_ctg_id\n",
    "    for ref_ctg_pos in sorted(per_ref_ctg[ref_ctg_id]):\n",
    "        replicates = len(per_ref_ctg[ref_ctg_id][ref_ctg_pos])\n",
    "        if replicates > 1:\n",
    "#            print \"\\nReference startpoint '%s' on reference ctg id '%s' is hit by %s queries -> filter\" %(ref_ctg_pos,ref_ctg_id,replicates)\n",
    "\n",
    "            matchid = ref_ctg_id+\"|%s\" %ref_ctg_pos\n",
    "            binned['position'][matchid] = []\n",
    "            for rep in per_ref_ctg[ref_ctg_id][ref_ctg_pos]:\n",
    "\n",
    "                binned['position'][matchid].append(reformat_match(ref_id=ref_ctg_id, ref_start_pos=ref_ctg_pos, baselist=rep))\n",
    "                \n",
    "                filtercount+=1\n",
    "                \n",
    "            del per_ref_ctg[ref_ctg_id][ref_ctg_pos]\n",
    "            if len(per_ref_ctg[ref_ctg_id]) == 0:\n",
    "                del per_ref_ctg[ref_ctg_id]\n",
    "\n",
    "                \n",
    "print \"Filter 'multiple query hits with identical start position on reference' removed %i Mummer hits\" %filtercount\n",
    "total-=filtercount\n",
    "print \"Leaves us with %i hits to process\" %total\n",
    "\n",
    "\n",
    "##extract uniques\n",
    "#print \"Number of contigs in dictionary before unique matches were removed: %s\" %len(per_ref_ctg)\n",
    "#identify the matches \n",
    "for ref_ctg_id in sorted(per_ref_ctg):\n",
    "#    print ref_ctg_id,len(per_ref_ctg[ref_ctg_id]),sorted(per_ref_ctg[ref_ctg_id])\n",
    "    if len(per_ref_ctg[ref_ctg_id]) == 1:\n",
    "        ref_ctg_pos = per_ref_ctg[ref_ctg_id].keys()[0]\n",
    "        temp = per_ref_ctg[ref_ctg_id][ref_ctg_pos][0]\n",
    "\n",
    "        \n",
    "        #check if distance from ctg start is ok\n",
    "#        if not ref_ctg_pos >= (dist_from_edge): #if match is not far enough from the start of the sequence\n",
    "        start = evaluate_dist_from_end(dist_from_edge=dist_from_edge, pos=ref_ctg_pos, mode='start')\n",
    "        end = evaluate_dist_from_end(dist_from_edge=dist_from_edge, pos=ref_ctg_pos, mode='end', total_length=lengths[ref_ctg_id])\n",
    "        if not start or not end:\n",
    "#        if not evaluate_dist_from_end(dist_from_edge=dist_from_edge, pos=ref_ctg_pos, mode='start'):\n",
    "            temp.append('too-close')\n",
    "#            print \"too close to edge - %s\" %ref_ctg_id\n",
    "            del per_ref_ctg[ref_ctg_id]\n",
    "            continue\n",
    "            \n",
    "        else: #move match to candidate status\n",
    "            \n",
    "            matchid = ref_ctg_id+\"|%s\" %ref_ctg_pos\n",
    "#            print \"sending to candidates -> %s\" %matchid\n",
    "\n",
    "            candidates['singlematch'][matchid] = reformat_match(ref_id=ref_ctg_id, ref_start_pos=ref_ctg_pos, baselist=temp)\n",
    "    \n",
    "#            print \"unique - %s\" %ref_ctg_id\n",
    "                    \n",
    "#        print \"%s\\n\" %candidates['singlematch'][matchid]\n",
    "\n",
    "        #remove match from original dictionary\n",
    "        del per_ref_ctg[ref_ctg_id]\n",
    "\n",
    "#    print \"more than one - %s\" %ref_ctg_id\n",
    "\n",
    "\n",
    "#print \"Number of contigs in dictionary after unique matches were removed: %s\" %len(per_ref_ctg)\n",
    "################################################\n",
    "\n",
    "to_merge = {}\n",
    "\n",
    "\n",
    "#Identify matches in close proximity\n",
    "#At this stage the per_ref_ctg dictionary contains exclusively reference contigs that had more than one match with the query, but none \n",
    "#that started at the same position (see filter 'multiple query hits with identical start position on reference' above)\n",
    "\n",
    "\n",
    "\n",
    "for ref_ctg_id in sorted(per_ref_ctg):\n",
    "    pos_list = sorted(per_ref_ctg[ref_ctg_id])\n",
    "#    print pos_list\n",
    "#    print ref_ctg_id,len(per_ref_ctg[ref_ctg_id]),sorted(per_ref_ctg[ref_ctg_id])\n",
    "#    print \"\\n### %s\" %ref_ctg_id #,len(per_ref_ctg[ref_ctg_id]),sorted(per_ref_ctg[ref_ctg_id])\n",
    "\n",
    "    for i in range(len(pos_list)-1):\n",
    "#        print per_ref_ctg[ref_ctg_id][pos_list[i]][0]\n",
    "        if len(per_ref_ctg[ref_ctg_id][pos_list[i]][0]) == 4:\n",
    "            if pos_list[i] >= (dist_from_edge):\n",
    "                per_ref_ctg[ref_ctg_id][pos_list[i]][0].append('unique') #only if it's the first match on a contig ][pos_list[i]][0][4]\n",
    "            else:\n",
    "                per_ref_ctg[ref_ctg_id][pos_list[i]][0].append('too-close')\n",
    "                \n",
    "#        print \"\\n[%i]: compare '%i' vs. %i\" %(i, pos_list[i], pos_list[i+1])\n",
    "\n",
    "        #calculate the distance between the current match [i] and the next match [i+1] on the reference contig = distance\n",
    "        current_length = int(per_ref_ctg[ref_ctg_id][pos_list[i]][0][2])\n",
    "        distance = pos_list[i+1] - (pos_list[i]+current_length)\n",
    "#        print \"%s: %s %i %s %s\" %(ref_ctg_id,pos_list[i],current_length, pos_list[i+1], distance)\n",
    "\n",
    "        #if the distance between two adjacent matches is larger than a defined value (unique_distance) the match will be considered as unique\n",
    "        if distance >= unique_distance:\n",
    "#            print \"Unique distance\"\n",
    "            per_ref_ctg[ref_ctg_id][pos_list[i+1]][0].append('unique')\n",
    "#            if per_ref_ctg[ref_ctg_id][pos_list[i]][0][4] == 'unique':\n",
    "#                print pos_list[i],per_ref_ctg[ref_ctg_id][pos_list[i]][0]\n",
    "#                continue\n",
    "        \n",
    "        elif distance < 0:\n",
    "#            print \"overlapping match -> exclude\"\n",
    "            per_ref_ctg[ref_ctg_id][pos_list[i]][0][4] = 'overlap-filter'\n",
    "            per_ref_ctg[ref_ctg_id][pos_list[i+1]][0].append('overlap-filter')\n",
    "        \n",
    "        elif distance > 0:\n",
    "            if distance <= max_merge_distance:\n",
    "#            print \"Matches within merge distance\"\n",
    "                if per_ref_ctg[ref_ctg_id][pos_list[i]][0][4] == 'unique':\n",
    "                    per_ref_ctg[ref_ctg_id][pos_list[i]][0][4] = 'merge-'+ref_ctg_id+'|'+str(pos_list[i])\n",
    "                    if not ref_ctg_id+'|'+str(pos_list[i]) in to_merge:\n",
    "                        to_merge[ref_ctg_id+'|'+str(pos_list[i])] = []\n",
    "                    to_merge[ref_ctg_id+'|'+str(pos_list[i])].append(pos_list[i])\n",
    "                    \n",
    "                    per_ref_ctg[ref_ctg_id][pos_list[i+1]][0].append('merge-'+ref_ctg_id+'|'+str(pos_list[i]))\n",
    "                    to_merge[ref_ctg_id+'|'+str(pos_list[i])].append(pos_list[i+1])\n",
    "                elif per_ref_ctg[ref_ctg_id][pos_list[i]][0][4].startswith('merge'): #merge label found -> append same merge label to next \n",
    "                    per_ref_ctg[ref_ctg_id][pos_list[i+1]][0].append(per_ref_ctg[ref_ctg_id][pos_list[i]][0][4])\n",
    "                    to_merge[per_ref_ctg[ref_ctg_id][pos_list[i]][0][4].replace('merge-','')].append(pos_list[i+1])\n",
    "                elif per_ref_ctg[ref_ctg_id][pos_list[i]][0][4] == 'overlap-filter':\n",
    "                    per_ref_ctg[ref_ctg_id][pos_list[i+1]][0].append('overlap-filter')\n",
    "                elif per_ref_ctg[ref_ctg_id][pos_list[i]][0][4] == 'too-close':\n",
    "                    per_ref_ctg[ref_ctg_id][pos_list[i+1]][0].append('too-close')\n",
    "                elif per_ref_ctg[ref_ctg_id][pos_list[i]][0][4] == 'skip':\n",
    "#                    print \"### SKIP CASE\"\n",
    "#                    print pos_list[i+1],per_ref_ctg[ref_ctg_id][pos_list[i+1]],pos_list[i-1],per_ref_ctg[ref_ctg_id][pos_list[i-1]]\n",
    "#                    print \"distance: %s vs. %s\" %(pos_list[i+1] - (pos_list[i-1]+int(per_ref_ctg[ref_ctg_id][pos_list[i-1]][0][2])), unique_distance)\n",
    "                    if (pos_list[i+1] - (pos_list[i-1]+int(per_ref_ctg[ref_ctg_id][pos_list[i-1]][0][2]))) >= unique_distance:\n",
    "                        per_ref_ctg[ref_ctg_id][pos_list[i+1]][0].append('unique')\n",
    "#                        print \"i+1 will be unique\"\n",
    "                    else:\n",
    "                        per_ref_ctg[ref_ctg_id][pos_list[i+1]][0].append('skip')\n",
    "#                        print \"i+1 will be skip\"\n",
    "                        \n",
    "            else:\n",
    "                if per_ref_ctg[ref_ctg_id][pos_list[i]][0][4] == 'unique' or per_ref_ctg[ref_ctg_id][pos_list[i]][0][4].startswith('merge'):\n",
    "                    per_ref_ctg[ref_ctg_id][pos_list[i+1]][0].append('skip')\n",
    "                elif per_ref_ctg[ref_ctg_id][pos_list[i]][0][4] == 'skip':\n",
    "#                    print \"### SKIP CASE 2\"\n",
    "#                    print pos_list[i+1],per_ref_ctg[ref_ctg_id][pos_list[i+1]],pos_list[i-1],per_ref_ctg[ref_ctg_id][pos_list[i-1]]\n",
    "#                    print \"distance: %s vs. %s\" %(pos_list[i+1] - (pos_list[i-1]+int(per_ref_ctg[ref_ctg_id][pos_list[i-1]][0][2])), unique_distance)\n",
    "                    if (pos_list[i+1] - (pos_list[i-1]+int(per_ref_ctg[ref_ctg_id][pos_list[i-1]][0][2]))) >= unique_distance:\n",
    "                        per_ref_ctg[ref_ctg_id][pos_list[i+1]][0].append('unique')\n",
    "#                        print \"i+1 will be unique 2\"\n",
    "                    else:\n",
    "                        per_ref_ctg[ref_ctg_id][pos_list[i+1]][0].append('skip')\n",
    "#                        print \"i+1 will be skip 2\"\n",
    "                else:\n",
    "                    per_ref_ctg[ref_ctg_id][pos_list[i+1]][0].append('unique')\n",
    "                                    \n",
    "                \n",
    "#        print pos_list[i],per_ref_ctg[ref_ctg_id][pos_list[i]][0]\n",
    "\n",
    "        if per_ref_ctg[ref_ctg_id][pos_list[i]][0][4] == 'unique':\n",
    "            matchid = ref_ctg_id+\"|%s\" %pos_list[i]\n",
    "            temp = per_ref_ctg[ref_ctg_id][pos_list[i]][0]\n",
    "#            print \"sending to candidates -> %s\" %matchid\n",
    "        \n",
    "            candidates['unique'][matchid] = reformat_match(ref_id=ref_ctg_id, ref_start_pos=pos_list[i], baselist=temp)\n",
    "            \n",
    "#            print \"%s\\n\" %candidates['unique'][matchid]\n",
    "    \n",
    "    #output the decision for the last match on the contig\n",
    "#    print \"[%i]: %i - %s\" %(len(pos_list), pos_list[-1], per_ref_ctg[ref_ctg_id][pos_list[-1]][0]) \n",
    "    if per_ref_ctg[ref_ctg_id][pos_list[-1]][0][4] == 'unique':\n",
    "        matchid = ref_ctg_id+\"|%s\" %pos_list[-1]\n",
    "        temp = per_ref_ctg[ref_ctg_id][pos_list[-1]][0]\n",
    "#        print \"sending to candidates -> %s\" %matchid\n",
    "\n",
    "        candidates['unique'][matchid] = reformat_match(ref_id=ref_ctg_id, ref_start_pos=pos_list[-1], baselist=temp)            \n",
    "#        print \"%s\\n\" %candidates['unique'][matchid]\n",
    "\n",
    "\n",
    "print \"\\nMERGING\\n\"\n",
    "\n",
    "#print to_merge\n",
    "\n",
    "for mergers in sorted(to_merge):\n",
    "    ok = True\n",
    "#    print \"\\n\"+mergers,to_merge[mergers]\n",
    "    ctg_id = \"|\".join(mergers.split(\"|\")[:-1])\n",
    "#    print ctg_id\n",
    "    temp = []\n",
    "    query_ids = []\n",
    "\n",
    "    for start in to_merge[mergers]:\n",
    "#        print start,per_ref_ctg[ctg_id][start][0]\n",
    "        temp.append(per_ref_ctg[ctg_id][start][0])\n",
    "        query_ids.append(per_ref_ctg[ctg_id][start][0][0])\n",
    "\n",
    "    if len(list(set(query_ids))) == 1: #this is true if all query matches are on the same contig\n",
    "#        print \"single queryID\"\n",
    "        pass\n",
    "        \n",
    "    else:\n",
    "#        print \"non unique query ctg\"\n",
    "        \n",
    "#        print \"\\n\"+mergers,to_merge[mergers]\n",
    "        binned['irregular-merge'][mergers] = []\n",
    "        for start2 in to_merge[mergers]:\n",
    "#            print ctg_id,start2,per_ref_ctg[ctg_id][start2][0]\n",
    "            binned['irregular-merge'][mergers].append(reformat_match(ref_id=ctg_id, ref_start_pos=start2, baselist=per_ref_ctg[ctg_id][start2][0]))\n",
    "\n",
    "        continue\n",
    "    \n",
    "    for i in range(len(temp)-1):\n",
    "#        print temp[i]\n",
    "        distance_ref = to_merge[mergers][i+1] - (to_merge[mergers][i] + int(temp[i][2]))\n",
    "        distance_que = int(temp[i+1][1]) - (int(temp[i][1]) + int(temp[i][2]))\n",
    "#        print \"REFERENCE DISTANCE: %i\" %distance_ref\n",
    "#        print \"QUERY DISTANCE: %i\" %distance_que\n",
    "\n",
    "#        diff = distance_que-distance_ref\n",
    "        diff = abs(distance_que-distance_ref)\n",
    "#        print \"DIFF: %i\\n\" %diff\n",
    "        \n",
    "#        if diff >= min_diff and diff <= max_diff:\n",
    "\n",
    "        if diff <= max_diff:\n",
    "            pass\n",
    "        else:\n",
    "#            print \"Diff conflict - diff = %i\" %diff\n",
    "            binned['merge-dist-diff'][mergers] = []\n",
    "            for start2 in to_merge[mergers]:\n",
    "                binned['merge-dist-diff'][mergers].append(reformat_match(ref_id=ctg_id, ref_start_pos=start2, baselist=per_ref_ctg[ctg_id][start2][0]))\n",
    "\n",
    "            ok = False\n",
    "            break\n",
    "    \n",
    "    if ok:\n",
    "#        print \"GO MERGE\"\n",
    "        pass\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    ##Merge\n",
    "    candidates['merged'][mergers] = {'ref':{}, 'que':{}}\n",
    "#    print candidates['merged'][mergers]\n",
    "    \n",
    "    candidates['merged'][mergers]['ref']['id'] = ctg_id\n",
    "    candidates['merged'][mergers]['ref']['start'] = to_merge[mergers][0]\n",
    "    candidates['merged'][mergers]['ref']['end'] = to_merge[mergers][-1] + int(temp[-1][2])\n",
    "    candidates['merged'][mergers]['ref']['seq'] = ''\n",
    "    candidates['merged'][mergers]['ref']['or'] = '+'\n",
    "    candidates['merged'][mergers]['que']['id'] = query_ids[0]\n",
    "    candidates['merged'][mergers]['que']['start'] = int(temp[0][1])\n",
    "    candidates['merged'][mergers]['que']['end'] = int(temp[-1][1]) + int(temp[-1][2])\n",
    "    candidates['merged'][mergers]['que']['seq'] = ''\n",
    "    candidates['merged'][mergers]['que']['or'] = '+'\n",
    "    \n",
    "    if query_ids[0].endswith('Reverse'):\n",
    "        candidates['merged'][mergers]['que']['or'] = '-'\n",
    "        candidates['merged'][mergers]['que']['id'] = query_ids[0].replace('_Reverse','')\n",
    "\n",
    "#    print candidates['merged'][mergers]\n",
    "        \n",
    "\n",
    "    \n",
    "## Extract merged sequences from fasta\n",
    "\n",
    "\n",
    "#identify ref contigs for which I need to extract sequences\n",
    "merge_ref_ctgs = {}\n",
    "for merged in candidates['merged']:\n",
    "    ref_ctg = \"|\".join(merged.split(\"|\")[:-1])\n",
    "    if not ref_ctg in merge_ref_ctgs:\n",
    "        merge_ref_ctgs[ref_ctg] = []\n",
    "    merge_ref_ctgs[ref_ctg].append(merged)\n",
    "    \n",
    "#for ctg in sorted(merge_ref_ctgs):\n",
    "#    print ctg,merge_ref_ctgs[ctg]\n",
    "    \n",
    "from Bio import SeqIO\n",
    "\n",
    "for seq in SeqIO.parse(open(reffasta,'r'),'fasta'):\n",
    "    if seq.id in merge_ref_ctgs:\n",
    "#        print seq.id\n",
    "        for uce in merge_ref_ctgs[seq.id]:\n",
    "            if candidates['merged'][uce]['ref']['or'] == '+':\n",
    "                uc_seq = str(seq.seq[candidates['merged'][uce]['ref']['start']-1:candidates['merged'][uce]['ref']['end']-1])\n",
    "            else:\n",
    "                uc_seq = str(seq.seq.reverse_complement()[candidates['merged'][uce]['ref']['start']-1:candidates['merged'][uce]['ref']['end']-1])\n",
    "#            print \"\\t%s,%s,%s\" %(uce, candidates['merged'][uce]['ref']['refstart'], candidates['merged'][uce]['ref']['refend'])\n",
    "            candidates['merged'][uce]['ref']['seq'] = uc_seq\n",
    "#            print candidates['merged'][uce]['ref']\n",
    "\n",
    "            \n",
    "#Identify query contigs for which I need to extract sequences\n",
    "merge_que_ctgs = {}\n",
    "for merged in candidates['merged']:\n",
    "    ctg = candidates['merged'][merged]['que']['id']\n",
    "    \n",
    "    if not ctg in merge_que_ctgs:\n",
    "        merge_que_ctgs[ctg] = []\n",
    "    merge_que_ctgs[ctg].append(merged)\n",
    "\n",
    "    \n",
    "#for ctg in sorted(merge_que_ctgs):\n",
    "#    print ctg,merge_que_ctgs[ctg]\n",
    "\n",
    "\n",
    "for seq in SeqIO.parse(open(quefasta,'r'),'fasta'):\n",
    "    if seq.id in merge_que_ctgs:\n",
    "#        print seq.id\n",
    "        for uce in merge_que_ctgs[seq.id]:\n",
    "            if candidates['merged'][uce]['que']['or'] == '+':\n",
    "                uc_seq = str(seq.seq[candidates['merged'][uce]['que']['start']-1:candidates['merged'][uce]['que']['end']-1])\n",
    "            else:\n",
    "                uc_seq = str(seq.seq.reverse_complement()[candidates['merged'][uce]['que']['start']-1:candidates['merged'][uce]['que']['end']-1])\n",
    "#            print \"\\t%s,%s,%s\" %(uce, candidates['merged'][uce]['que']['start'], candidates['merged'][uce]['que']['end'])\n",
    "            candidates['merged'][uce]['que']['seq'] = uc_seq\n",
    "\n",
    "#            print \"%s\\n%s\" %(candidates['merged'][uce]['ref']['seq'],candidates['merged'][uce]['que']['seq'])\n",
    "            \n",
    "            #test\n",
    "#            ctg_id = \"|\".join(uce.split(\"|\")[:-1])\n",
    "#            for start in to_merge[uce]:\n",
    "#                print per_ref_ctg[ctg_id][start][0][3]\n",
    "            \n",
    "\n",
    "#move invalid matches to bin\n",
    "for bingroup in ['too-close','overlap-filter','skip']:\n",
    "#    print bingroup\n",
    "    for ref_ctg_id in sorted(per_ref_ctg):\n",
    "    \n",
    "        for start in per_ref_ctg[ref_ctg_id]:\n",
    "#            print bingroup,ref_ctg_id,start\n",
    "#            print per_ref_ctg[ref_ctg_id][start][0]\n",
    "            temp = per_ref_ctg[ref_ctg_id][start][0]\n",
    "            \n",
    "            if bingroup in temp:\n",
    "                matchid = ref_ctg_id+\"|%s\" %start\n",
    "                \n",
    "                binned[bingroup][matchid] = reformat_match(ref_id=ref_ctg_id, ref_start_pos=start, baselist=temp)\n",
    "\n",
    "                \n",
    "#output summary for binned matches\n",
    "print\n",
    "for bingroup in binned:\n",
    "    print \"%s\\t%s\" %(bingroup,len(binned[bingroup]))\n",
    "    \n",
    "##summarize candidates\n",
    "print\n",
    "for group in candidates.keys():\n",
    "    print \"%s\\t%s\" %(group,len(candidates[group]))\n",
    "        \n",
    "\n",
    "#output length distribution per candidate group\n",
    "for group in candidates.keys():\n",
    "    print \"\\n%s\\t%s\" %(group,len(candidates[group]))\n",
    "    length_dist = {}\n",
    "    for uce in candidates[group]:\n",
    "        length = candidates[group][uce]['ref']['end'] - candidates[group][uce]['ref']['start']\n",
    "        if not length in length_dist:\n",
    "            length_dist[length] = 0\n",
    "        length_dist[length]+=1\n",
    "    for l in sorted(length_dist):\n",
    "        print l,length_dist[l]\n",
    "#    print candidates[group]\n",
    "\n",
    "\n",
    "\n",
    "##CHECK distance from contig ends for singlematches too\n",
    "\n",
    "print \"\\nWRITING GLOBAL OUTPUTS with prefix: '%s'\\n\" %global_prefix\n",
    "\n",
    "write_binned_match_details(global_prefix+'.bin.reason.tsv', master_dict=binned)\n",
    "\n",
    "write_UCE_coordinates(global_prefix+'.candidate_coordinates.tsv', master_dict=candidates)\n",
    "\n",
    "\n",
    "name_body = '.candidate_UCEs'\n",
    "if minlength:\n",
    "    name_body += '-minlength'+str(minlength)\n",
    "        \n",
    "#Extract only UCE's from reference with minimum length of 100\n",
    "print \"\\nWRITING: %s%s.fasta\" %(ref_prefix,name_body)\n",
    "extract_UCEs(prefix = ref_prefix+name_body, candidates=candidates, target='ref', minlength=minlength)\n",
    "#Extract only UCE's from query with minimum length of 100\n",
    "print \"\\nWRITING: %s%s.fasta\" %(que_prefix,name_body)\n",
    "extract_UCEs(prefix = que_prefix+name_body, candidates=candidates, target='que', minlength=minlength)\n",
    "\n",
    "if extension:\n",
    "    name_body += '-extended'+str(extension)\n",
    "    #Extract UCE's from reference with minimum length of 100 with 500 bp extension\n",
    "    print \"\\nWRITING: %s%s.fasta\" %(ref_prefix,name_body)\n",
    "    extract_UCEs(prefix = ref_prefix+name_body, candidates=candidates, target='ref', minlength=minlength, extension=extension, fasta=reffasta)\n",
    "    #Extract UCE's from query with minimum length of 100 with 500 bp extension\n",
    "    print \"\\nWRITING: %s%s.fasta\" %(que_prefix,name_body)\n",
    "    extract_UCEs(prefix = que_prefix+name_body, candidates=candidates, target='que', minlength=minlength, extension=extension, fasta=quefasta)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine both output files and sort by reference contig and start position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat LvSf80.out.candidate_coordinates.tsv LvSf80.out.bin.reason.tsv | sort -k 3,3 -k 4,4n > complete_report.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output cummulative length distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\t1214\n",
      "90\t738\n",
      "100\t487\n",
      "110\t314\n",
      "120\t212\n",
      "130\t141\n",
      "140\t109\n",
      "150\t84\n",
      "160\t68\n",
      "170\t56\n",
      "180\t45\n",
      "190\t40\n",
      "200\t27\n",
      "210\t24\n",
      "220\t18\n",
      "230\t15\n",
      "240\t13\n",
      "250\t10\n",
      "260\t9\n",
      "270\t9\n",
      "280\t8\n",
      "290\t7\n",
      "300\t7\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "file=Sfran.candidate_UCEs.fasta\n",
    "\n",
    "\n",
    "for i in {80..300..10}\n",
    "do \n",
    "    cat <(echo $i) $file | \\\n",
    "    grep \">\" -v | \\\n",
    "    perl -ne 'chomp; if ($. == 1){ $min = $_; $count=0;}else{if (length($_) >= $min){$count++; print \"$min\\t$count\\n\"}}' | \\\n",
    "    tail -n 1\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\t29\n",
      "90\t29\n",
      "100\t29\n",
      "110\t29\n",
      "120\t29\n",
      "130\t29\n",
      "140\t29\n",
      "150\t29\n",
      "160\t29\n",
      "170\t29\n",
      "180\t26\n",
      "190\t24\n",
      "200\t17\n",
      "210\t16\n",
      "220\t10\n",
      "230\t8\n",
      "240\t6\n",
      "250\t6\n",
      "260\t5\n",
      "270\t5\n",
      "280\t5\n",
      "290\t4\n",
      "300\t4\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "file=Sfran.candidate_UCEs.fasta\n",
    "\n",
    "##cummulative length distribution only for merged UCE's\n",
    "for i in {80..300..10}\n",
    "do \n",
    "    cat <(echo $i) <(cat $file | grep \"merged\" -A 1 | grep \">\" -v) | \\\n",
    "    perl -ne 'chomp; if ($. == 1){ $min = $_; $count=0;}else{if (length($_) >= $min){$count++; print \"$min\\t$count\\n\"}}' | \\\n",
    "    tail -n 1\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The code has been transferred to a python script (01.02.2017) and is running stably now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
